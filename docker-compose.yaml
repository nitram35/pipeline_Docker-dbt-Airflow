version: "3"

services:
  source_postgres:
    image: postgres:15
    ports:
      - "5433:5432"
    networks:
      - elt_network
    environment:
      POSTGRES_DB: source_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: secret
    volumes:
      - ./source_db_init/init.sql:/docker-entrypoint-initdb.d/init.sql

  destination_postgres:
    image: postgres:15
    ports:
      - "5434:5432"
    networks:
      - elt_network
    environment:
      POSTGRES_DB: destination_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: secret

  # We do not need to create this containers anymore because we have airflow doing it now (check airflow directory)

  # elt_script:
  #   build:
  #     context: ./elt # Directory containing the Dockerfile and elt_script.py
  #     dockerfile: Dockerfile # Name of the Dockerfile, if it's something other than "Dockerfile", specify here
  #   command: ["python", "elt_script.py"]
  #   networks:
  #     - elt_network
  #   # Depends on the source and destination databases, it will wait for them to be ready before starting
  #   depends_on:
  #     - source_postgres
  #     - destination_postgres

  # dbt:
  #   image: ghcr.io/dbt-labs/dbt-postgres:1.4.7
  #   command: ["run", "--profiles-dir", "/root", "--project-dir", "/dbt"]
  #   networks:
  #     - elt_network
  #   volumes:
  #     - ./custom_postgres:/dbt
  #     - ~/.dbt:/root
  #   depends_on:
  #     elt_script:
  #       condition: service_completed_successfully

  #   environment:
  #     DBT_PROFILE: default
  #     DBT_TARGET: dev

  # For airflow we have a bunch of services to add
  postgres:
    image: postgres:15
    networks:
      - elt_network
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow

  init-airflow:
    image: apache/airflow:latest
    depends_on:
      - postgres
    networks:
      - elt_network
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    command: >
      bash -c "airflow db init && 
              airflow users create --username airflow --password password --firstname John --lastname Doe --role Admin --email admin@example.com"

  # We create a Dockerfile for this one, the webserver gives us a UI to interact with the DAGs
  webserver:
    build:
      context: .
      dockerfile: Dockerfile
    user: root
    depends_on:
      - postgres
    networks:
      - elt_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./elt:/opt/airflow/elt
      - ./custom_postgres:/opt/dbt
      - ~/.dbt:/root/.dbt
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW_CONN_DESTINATION_POSTGRES=postgres://postgres:secret@destination_postgres:5434/destination_db
      - AIRFLOW__CORE__FERNET_KEY=PCgIeIBHjLimoF39u3Ehab5ApwhnFONTvItEI0nNAc4=
      # a fernet key is a secret key used to encrypt the data in the database
      #  We use this commande to generate one --> python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
      - AIRFLOW__WEBSERVER__DEFAULT_USER_USERNAME=airflow
      - AIRFLOW__WEBSERVER__DEFAULT_USER_PASSWORD=password
      - AIRFLOW__WEBSERVER__SECRET_KEY=secret
      - AIRFLOW_WWW_USER_USERNAME=airflow
      - AIRFLOW_WWW_USER_PASSWORD=password
    ports:
      - "8080:8080"
    command: webserver

  # The scheduler is the same as the webserver, it's used to schedule all our jobs (DAGs)
  scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    user: root
    depends_on:
      - postgres
    networks:
      - elt_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./elt:/opt/airflow/elt
      - ./custom_postgres:/dbt
      - ~/.dbt:/root/.dbt
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW_CONN_DESTINATION_POSTGRES=postgres://postgres:secret@destination_postgres:5434/destination_db
      - AIRFLOW__CORE__FERNET_KEY=PCgIeIBHjLimoF39u3Ehab5ApwhnFONTvItEI0nNAc4=
      # a fernet key is a secret key used to encrypt the data in the database
      #  We use this commande to generate one --> python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
      - AIRFLOW__WEBSERVER__SECRET_KEY=secret
      - AIRFLOW_WWW_USER_USERNAME=airflow
      - AIRFLOW_WWW_USER_PASSWORD=password
    command: scheduler

networks:
  elt_network:
    driver: bridge
